{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from argparse import  Namespace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build MVCL model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MVCL import MultiViewModel_lit, MultiViewModel\n",
    "# pip install -r requirements.txt --cache-dir D:\\learn\\python-lib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Configuration Parameters\n",
    "\n",
    "| Parameter | Type | Default | Description |\n",
    "|-----------|------|---------|-------------|\n",
    "| `use_inner_CL` | int | 1 | Enable **inner contrastive learning** within the same modality for feature discrimination |\n",
    "| `use_inter_CL` | int | 1 | Enable **inter-modal contrastive learning** between different modalities |\n",
    "| `use_cls_loss_1_2` | int | 1 | Enable classification loss for modality 1 and modality 2 tasks |\n",
    "| `use_fusion` | int | 1 | Enable **feature fusion mechanism** to combine multi-modal features |\n",
    "| `use_fusion1D` | int | 1 | Enable **1D fusion** strategy for processing sequential feature fusion |\n",
    "| `use_fusion2D` | int | 1 | Enable **2D fusion** strategy for processing spatial feature map fusion |\n",
    "| `use_mse_loss` | int | 0 | Enable Mean Squared Error loss for regression tasks |\n",
    "| `only_1D` | int | 0 | **Use only 1D modality**, ignoring other dimensional features |\n",
    "| `only_2D` | int | 0 | **Use only 2D modality**, ignoring other dimensional features |\n",
    "| `drop_layer` | float | 0.0 | Dropout rate for regularization to prevent overfitting |\n",
    "| `w_con` | float | 1.0 | Weight coefficient for contrastive learning loss in total loss |\n",
    "| `w_cls` | float | 1.0 | Weight coefficient for classification loss in total loss |\n",
    "\n",
    "### Parameter Categories\n",
    "\n",
    "#### üéØ **Loss Function Control**\n",
    "- `use_inner_CL`, `use_inter_CL`: Control different types of contrastive learning\n",
    "- `use_cls_loss_1_2`: Control classification loss\n",
    "- `use_mse_loss`: Control regression loss\n",
    "\n",
    "#### üîÑ **Feature Fusion Strategy**\n",
    "- `use_fusion`: Master switch for feature fusion\n",
    "- `use_fusion1D`, `use_fusion2D`: Control fusion methods for different dimensions\n",
    "\n",
    "#### üéõÔ∏è **Modality Selection**\n",
    "- `only_1D`, `only_2D`: Control whether to use only specific dimensional modalities\n",
    "\n",
    "#### ‚öñÔ∏è **Weight Balancing**\n",
    "- `w_con`, `w_cls`: Balance the importance of different loss functions\n",
    "- `drop_layer`: Regularization parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### default model configuration\n",
    "mvcl_cfg = Namespace(\n",
    "    use_inner_CL=1,\n",
    "    use_inter_CL=1,\n",
    "    use_cls_loss_1_2=1,\n",
    "    use_fusion=1,\n",
    "    use_fusion1D=1,\n",
    "    use_fusion2D=1,\n",
    "    use_mse_loss=0,\n",
    "    only_1D=0,\n",
    "    only_2D=0,\n",
    "    drop_layer=0.0,\n",
    "    w_con=1.0,\n",
    "    w_cls=1.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first building, this will download the Wav2Clip model checkpoints and the WavLM model checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "992151677057413fa8029fa78c1486a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  44%|####4     | 168M/378M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramFiles\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\54572\\.cache\\huggingface\\hub\\models--microsoft--wavlm-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "mvcl = MultiViewModel(cfg=mvcl_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input of our MVCL model is a torch Tensor with shape of (batch, 1, audio_length).\n",
    "\n",
    "Take an random tensor as example. The batch size is 2, indicating this output is from processing 2 audio samples simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(2, 1, 48000)\n",
    "res = mvcl(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw_spec: torch.Size([2, 1, 257, 257])\n",
      "raw_wav_feat: torch.Size([2, 149, 768])\n",
      "feature1D: torch.Size([2, 768])\n",
      "feature2D: torch.Size([2, 512])\n",
      "feature: torch.Size([2, 1280])\n",
      "logit1D: torch.Size([2])\n",
      "logit2D: torch.Size([2])\n",
      "logit: torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "for k, v in res.items():\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        print(f\"{k}: {v.shape}\")\n",
    "    else:\n",
    "        print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is a dict:\n",
    "\n",
    "| Feature Name | Shape | Dimension | Description |\n",
    "|--------------|-------|-----------|-------------|\n",
    "| `raw_spec` | `[2, 1, 257, 257]` | 4D | **Raw spectrogram** - Original frequency-time representation of audio signal with 257 frequency bins and 257 time frames |\n",
    "| `raw_wav_feat` | `[2, 149, 768]` | 3D | **Raw waveform features** - Sequential audio features extracted from backbone (e.g., WavLM), 149 time steps with 768-dimensional embeddings |\n",
    "| `feature1D` | `[2, 768]` | 2D | **1D modality features** - the final classification feat of the 1D branch |\n",
    "| `feature2D` | `[2, 512]` | 2D | **2D modality features** - the final classification feat of the 2D branch |\n",
    "| `feature` | `[2, 1280]` | 2D | **Fused features** - Combined multi-modal features (1D + 2D), concatenated to 1280 dimensions (768 + 512) |\n",
    "| `logit1D` | `[2]` | 1D | **1D modality logits** - Classification scores from 1D feature branch for binary classification |\n",
    "| `logit2D` | `[2]` | 1D | **2D modality logits** - Classification scores from 2D feature branch for binary classification |\n",
    "| `logit` | `[2]` | 1D | **Final logits** - Combined classification scores from fused features for final prediction |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Feature Processing Pipeline**\n",
    "\n",
    "```\n",
    "Audio Input (batch, 1, 48000)\n",
    "    ‚Üì\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                    Stage 1 (No Grad)                       ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ      1D Branch           ‚îÇ           2D Branch              ‚îÇ\n",
    "‚îÇ                          ‚îÇ                                  ‚îÇ\n",
    "‚îÇ feature_model1D          ‚îÇ feature_model2D                  ‚îÇ\n",
    "‚îÇ .compute_stage1(x)       ‚îÇ .compute_stage1(x, spec_aug)     ‚îÇ\n",
    "‚îÇ      ‚Üì                   ‚îÇ      ‚Üì                           ‚îÇ\n",
    "‚îÇ    wav1                  ‚îÇ   spec1, raw_spec               ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                           ‚Üì\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                 Cross-Modal Fusion                          ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ   squeeze_modules[0]     ‚îÇ     expand_modules[0]            ‚îÇ\n",
    "‚îÇ   (wav1, spec1)          ‚îÇ     (wav1, spec1)                ‚îÇ\n",
    "‚îÇ      ‚Üì                   ‚îÇ      ‚Üì                           ‚îÇ\n",
    "‚îÇ   fused_wav1             ‚îÇ   fused_spec1                    ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                           ‚Üì\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                    Stage 2                                  ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ feature_model1D          ‚îÇ feature_model2D                  ‚îÇ\n",
    "‚îÇ .compute_stage2          ‚îÇ .compute_stage2                  ‚îÇ\n",
    "‚îÇ (fused_wav1)             ‚îÇ (fused_spec1)                    ‚îÇ\n",
    "‚îÇ      ‚Üì                   ‚îÇ      ‚Üì                           ‚îÇ\n",
    "‚îÇ wav2, position_bias      ‚îÇ   spec2                          ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                           ‚Üì\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ              Cross-Modal Fusion + Stage 3                   ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ   squeeze_modules[1]     ‚îÇ     expand_modules[1]            ‚îÇ\n",
    "‚îÇ   (wav2, spec2)          ‚îÇ     (wav2, spec2)                ‚îÇ\n",
    "‚îÇ      ‚Üì                   ‚îÇ      ‚Üì                           ‚îÇ\n",
    "‚îÇ feature_model1D          ‚îÇ feature_model2D                  ‚îÇ\n",
    "‚îÇ .compute_stage3          ‚îÇ .compute_stage3                  ‚îÇ\n",
    "‚îÇ      ‚Üì                   ‚îÇ      ‚Üì                           ‚îÇ\n",
    "‚îÇ wav3, position_bias      ‚îÇ   spec3                          ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                           ‚Üì\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ              Cross-Modal Fusion + Stage 4                   ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ   squeeze_modules[2]     ‚îÇ     expand_modules[2]            ‚îÇ\n",
    "‚îÇ   (wav3, spec3)          ‚îÇ     (wav3, spec3)                ‚îÇ\n",
    "‚îÇ      ‚Üì                   ‚îÇ      ‚Üì                           ‚îÇ\n",
    "‚îÇ feature_model1D          ‚îÇ feature_model2D                  ‚îÇ\n",
    "‚îÇ .compute_stage4          ‚îÇ .compute_stage4                  ‚îÇ\n",
    "‚îÇ      ‚Üì                   ‚îÇ      ‚Üì                           ‚îÇ\n",
    "‚îÇ wav4, position_bias      ‚îÇ   spec4                          ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                           ‚Üì\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ           Final Cross-Modal Fusion + Latent Features        ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ   squeeze_modules[3]     ‚îÇ     expand_modules[3]            ‚îÇ\n",
    "‚îÇ   (wav4, spec4)          ‚îÇ     (wav4, spec4)                ‚îÇ\n",
    "‚îÇ      ‚Üì                   ‚îÇ      ‚Üì                           ‚îÇ\n",
    "‚îÇ feature_model1D          ‚îÇ feature_model2D                  ‚îÇ\n",
    "‚îÇ .compute_latent_feature  ‚îÇ .compute_latent_feature          ‚îÇ\n",
    "‚îÇ      ‚Üì                   ‚îÇ      ‚Üì                           ‚îÇ\n",
    "‚îÇ wav5, raw_wav_feat       ‚îÇ   spec5                          ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                           ‚Üì\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                Feature Normalization                        ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ   norm_feat(wav5)        ‚îÇ   norm_feat(spec5)               ‚îÇ\n",
    "‚îÇ      ‚Üì                   ‚îÇ      ‚Üì                           ‚îÇ\n",
    "‚îÇ   feature1D [B, 768]     ‚îÇ   feature2D [B, 512]             ‚îÇ\n",
    "‚îÇ      ‚Üì                   ‚îÇ      ‚Üì                           ‚îÇ\n",
    "‚îÇ   cls1D(feature1D)       ‚îÇ   cls2D(feature2D)               ‚îÇ\n",
    "‚îÇ      ‚Üì                   ‚îÇ      ‚Üì                           ‚îÇ\n",
    "‚îÇ   logit1D [B]            ‚îÇ   logit2D [B]                    ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                           ‚Üì\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                   Multi-Modal Fusion                        ‚îÇ\n",
    "‚îÇ                                                             ‚îÇ\n",
    "‚îÇ        concat([wav5, spec5], dim=-1)                        ‚îÇ\n",
    "‚îÇ                     ‚Üì                                       ‚îÇ\n",
    "‚îÇ              norm_feat(concat)                              ‚îÇ\n",
    "‚îÇ                     ‚Üì                                       ‚îÇ\n",
    "‚îÇ               feature [B, 1280]                             ‚îÇ\n",
    "‚îÇ                     ‚Üì                                       ‚îÇ\n",
    "‚îÇ              cls_final(feature)                             ‚îÇ\n",
    "‚îÇ                     ‚Üì                                       ‚îÇ\n",
    "‚îÇ                logit [B]                                    ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lit model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> PyTorch Lightning is the deep learning framework for professional AI researchers and machine learning engineers who need maximal flexibility without sacrificing performance at scale. Lightning evolves with you as your projects go from idea to paper/production\n",
    "\n",
    "We use [pytorch_lightning](https://lightning.ai/docs/pytorch/stable/) to train, validate, and test our model. Besides, it can also easily control the logging, model saving and callbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import Trainer, LightningModule\n",
    "from pytorch_lightning.loggers import  CSVLogger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the pytorch Lightning module to train the model, where we define the train step, validation/predict step, loss function and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /microsoft/wavlm-base/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001C971669310>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 23a793c8-5133-4ecc-a22f-c93b7a2aa23f)')' thrown while requesting HEAD https://huggingface.co/microsoft/wavlm-base/resolve/main/config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /microsoft/wavlm-base/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001C95642F110>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: ef663dc0-e7ff-45a6-952f-2988c19f4af2)')' thrown while requesting HEAD https://huggingface.co/microsoft/wavlm-base/resolve/main/config.json\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /microsoft/wavlm-base/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001C95ABF9A90>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 851ea9ed-a255-44a8-a0e8-15733d119635)')' thrown while requesting HEAD https://huggingface.co/microsoft/wavlm-base/resolve/main/config.json\n",
      "Retrying in 4s [Retry 3/5].\n",
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /microsoft/wavlm-base/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001C95ABF9E50>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 5a17a899-dfb9-4ad3-a04e-93f48b781d76)')' thrown while requesting HEAD https://huggingface.co/microsoft/wavlm-base/resolve/main/config.json\n",
      "Retrying in 8s [Retry 4/5].\n",
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /microsoft/wavlm-base/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001C95ABF96D0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 720efcab-020c-4b0f-8403-0c6f8d077670)')' thrown while requesting HEAD https://huggingface.co/microsoft/wavlm-base/resolve/main/config.json\n",
      "Retrying in 8s [Retry 5/5].\n",
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /microsoft/wavlm-base/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001C95AD9A350>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 70e9946f-8605-4e7e-b171-048b4f3b3117)')' thrown while requesting HEAD https://huggingface.co/microsoft/wavlm-base/resolve/main/config.json\n",
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /microsoft/wavlm-base/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001C9716696D0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 81c5a7c4-a7ed-4ea2-9c40-84abc115ae03)')' thrown while requesting HEAD https://huggingface.co/microsoft/wavlm-base/resolve/main/config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /microsoft/wavlm-base/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001C971669950>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 82023642-ec34-4501-beb0-3deaad3be251)')' thrown while requesting HEAD https://huggingface.co/microsoft/wavlm-base/resolve/main/config.json\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /microsoft/wavlm-base/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001C971669BD0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 58cd959a-485d-4376-9ad4-40f705d8463c)')' thrown while requesting HEAD https://huggingface.co/microsoft/wavlm-base/resolve/main/config.json\n",
      "Retrying in 4s [Retry 3/5].\n",
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /microsoft/wavlm-base/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001C971669E50>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: c3e23390-93de-4b9f-a0e2-083ae24c0f5f)')' thrown while requesting HEAD https://huggingface.co/microsoft/wavlm-base/resolve/main/config.json\n",
      "Retrying in 8s [Retry 4/5].\n",
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /microsoft/wavlm-base/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001C97166A0D0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 52c1bacd-3edd-460b-9179-46ba702816d9)')' thrown while requesting HEAD https://huggingface.co/microsoft/wavlm-base/resolve/main/config.json\n",
      "Retrying in 8s [Retry 5/5].\n",
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /microsoft/wavlm-base/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001C97166A350>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 819b21f3-f973-472c-a142-08fb49baf0cc)')' thrown while requesting HEAD https://huggingface.co/microsoft/wavlm-base/resolve/main/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BCE loss with label smoothing:  0.1\n"
     ]
    }
   ],
   "source": [
    "mvcl_lit = MultiViewModel_lit(cfg=mvcl_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test forwarding "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lit model, we use the `_shared_pred` method to predict the logits of the input batch. If the stage is train, we also the the audio_transform to augment the spectrogram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a random batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = {\n",
    "    \"label\": torch.randint(0, 2, (3,)),\n",
    "    \"audio\": torch.randn(3, 1, 48000),\n",
    "    \"sample_rate\": [16000, 16000, 16000],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, **your batch must be a dict with above keys**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, the `_shared_pred` output is also a dict. We use it to compute the loss\n",
    "function, AUC, and ERR scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw_spec torch.Size([3, 1, 257, 257])\n",
      "raw_wav_feat torch.Size([3, 149, 768])\n",
      "feature1D torch.Size([3, 768])\n",
      "feature2D torch.Size([3, 512])\n",
      "feature torch.Size([3, 1280])\n",
      "logit1D torch.Size([3])\n",
      "logit2D torch.Size([3])\n",
      "logit torch.Size([3])\n",
      "pred torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "batch_res = mvcl_lit._shared_pred(batch=batch, batch_idx=0)\n",
    "for key, value in batch_res.items():\n",
    "    print(key, value.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first build a simple dataloaders for training, where all the samples are randomly generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from callbacks import EER_Callback, BinaryAUC_Callback, BinaryACC_Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a dataloader with random values for demo training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTestDataset(Dataset):\n",
    "    def __init__(self, num_samples=10):\n",
    "        # Generate synthetic data similar to your example\n",
    "        self.samples = []\n",
    "        for _ in range(num_samples):\n",
    "            self.samples.append({\n",
    "                \"audio\": torch.randn(1, 48000),\n",
    "                \"label\": torch.randint(0, 2, (1,)).item(),\n",
    "                \"sample_rate\": 16000,\n",
    "            })\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n",
    "\n",
    "\n",
    "\n",
    "# Create the dataset and dataloader\n",
    "train_dataloader = DataLoader(\n",
    "    SimpleTestDataset(num_samples=100),\n",
    "    batch_size=3,\n",
    "    shuffle=True,\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    SimpleTestDataset(num_samples=50),\n",
    "    batch_size=3,\n",
    "    shuffle=False,\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    SimpleTestDataset(num_samples=20),\n",
    "    batch_size=3,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build a simple trainer to train and test our model, which uses:\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "MisconfigurationException",
     "evalue": "No supported gpu backend found!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMisconfigurationException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m      2\u001b[0m     logger\u001b[38;5;241m=\u001b[39mCSVLogger(save_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./logs\u001b[39m\u001b[38;5;124m\"\u001b[39m, version\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m),\n\u001b[0;32m      3\u001b[0m     max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[0;32m      4\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m      5\u001b[0m         BinaryACC_Callback(batch_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m, output_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m      6\u001b[0m         BinaryAUC_Callback(batch_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m, output_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m      7\u001b[0m         EER_Callback(batch_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m, output_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m      8\u001b[0m     ],\n\u001b[0;32m      9\u001b[0m     devices\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;66;03m# use cuda:0 device\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     accelerator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;66;03m# use GPU acceleration\u001b[39;00m\n\u001b[0;32m     11\u001b[0m )\n",
      "File \u001b[1;32md:\\ProgramFiles\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\utilities\\argparse.py:70\u001b[0m, in \u001b[0;36m_defaults_from_env_vars.<locals>.insert_env_defaults\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mlist\u001b[39m(env_variables\u001b[38;5;241m.\u001b[39mitems()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(kwargs\u001b[38;5;241m.\u001b[39mitems()))\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# all args were already moved to kwargs\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\ProgramFiles\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:404\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[1;34m(self, accelerator, strategy, devices, num_nodes, precision, logger, callbacks, fast_dev_run, max_epochs, min_epochs, max_steps, min_steps, max_time, limit_train_batches, limit_val_batches, limit_test_batches, limit_predict_batches, overfit_batches, val_check_interval, check_val_every_n_epoch, num_sanity_val_steps, log_every_n_steps, enable_checkpointing, enable_progress_bar, enable_model_summary, accumulate_grad_batches, gradient_clip_val, gradient_clip_algorithm, deterministic, benchmark, inference_mode, use_distributed_sampler, profiler, detect_anomaly, barebones, plugins, sync_batchnorm, reload_dataloaders_every_n_epochs, default_root_dir, model_registry)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;66;03m# init connectors\u001b[39;00m\n\u001b[0;32m    402\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_connector \u001b[38;5;241m=\u001b[39m _DataConnector(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 404\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accelerator_connector \u001b[38;5;241m=\u001b[39m _AcceleratorConnector(\n\u001b[0;32m    405\u001b[0m     devices\u001b[38;5;241m=\u001b[39mdevices,\n\u001b[0;32m    406\u001b[0m     accelerator\u001b[38;5;241m=\u001b[39maccelerator,\n\u001b[0;32m    407\u001b[0m     strategy\u001b[38;5;241m=\u001b[39mstrategy,\n\u001b[0;32m    408\u001b[0m     num_nodes\u001b[38;5;241m=\u001b[39mnum_nodes,\n\u001b[0;32m    409\u001b[0m     sync_batchnorm\u001b[38;5;241m=\u001b[39msync_batchnorm,\n\u001b[0;32m    410\u001b[0m     benchmark\u001b[38;5;241m=\u001b[39mbenchmark,\n\u001b[0;32m    411\u001b[0m     use_distributed_sampler\u001b[38;5;241m=\u001b[39muse_distributed_sampler,\n\u001b[0;32m    412\u001b[0m     deterministic\u001b[38;5;241m=\u001b[39mdeterministic,\n\u001b[0;32m    413\u001b[0m     precision\u001b[38;5;241m=\u001b[39mprecision,\n\u001b[0;32m    414\u001b[0m     plugins\u001b[38;5;241m=\u001b[39mplugins,\n\u001b[0;32m    415\u001b[0m )\n\u001b[0;32m    416\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger_connector \u001b[38;5;241m=\u001b[39m _LoggerConnector(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    417\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_connector \u001b[38;5;241m=\u001b[39m _CallbackConnector(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32md:\\ProgramFiles\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:144\u001b[0m, in \u001b[0;36m_AcceleratorConnector.__init__\u001b[1;34m(self, devices, num_nodes, accelerator, strategy, plugins, precision, sync_batchnorm, benchmark, use_distributed_sampler, deterministic)\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accelerator_flag \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_choose_auto_accelerator()\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accelerator_flag \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 144\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accelerator_flag \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_choose_gpu_accelerator_backend()\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_device_config_and_set_final_flags(devices\u001b[38;5;241m=\u001b[39mdevices, num_nodes\u001b[38;5;241m=\u001b[39mnum_nodes)\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_parallel_devices_and_init_accelerator()\n",
      "File \u001b[1;32md:\\ProgramFiles\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:354\u001b[0m, in \u001b[0;36m_AcceleratorConnector._choose_gpu_accelerator_backend\u001b[1;34m()\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m CUDAAccelerator\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[0;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 354\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo supported gpu backend found!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mMisconfigurationException\u001b[0m: No supported gpu backend found!"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    logger=CSVLogger(save_dir=\"./logs\", version=0),\n",
    "    max_epochs=4,\n",
    "    callbacks=[\n",
    "        BinaryACC_Callback(batch_key=\"label\", output_key=\"logit\"),\n",
    "        BinaryAUC_Callback(batch_key=\"label\", output_key=\"logit\"),\n",
    "        EER_Callback(batch_key=\"label\", output_key=\"logit\"),\n",
    "    ],\n",
    "    devices=[0], # use cuda:0 device\n",
    "    accelerator=\"gpu\", # use GPU acceleration\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "/home/ay/anaconda3/envs/mvcl/lib/python3.9/site-packages/lightning_fabric/loggers/csv_logs.py:268: Experiment logs directory ./logs/lightning_logs/version_0 exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!\n",
      "/home/ay/anaconda3/envs/mvcl/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:658: Checkpoint directory ./logs/lightning_logs/version_0/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name                | Type                            | Params | Mode \n",
      "--------------------------------------------------------------------------------\n",
      "0 | model               | MultiViewModel                  | 128 M  | train\n",
      "1 | clip_heads          | ModuleList                      | 1.6 M  | train\n",
      "2 | bce_loss            | LabelSmoothingBCE               | 0      | train\n",
      "3 | contrast_loss2      | BinaryTokenContrastLoss         | 0      | train\n",
      "4 | triplet_loss        | TripletMarginLoss               | 0      | train\n",
      "5 | clip_loss           | CLIPLoss1D                      | 1      | train\n",
      "6 | reconstruction_loss | TimeFrequencyReconstructionLoss | 379 K  | train\n",
      "--------------------------------------------------------------------------------\n",
      "130 M     Trainable params\n",
      "0         Non-trainable params\n",
      "130 M     Total params\n",
      "521.061   Total estimated model params size (MB)\n",
      "210       Modules in train mode\n",
      "233       Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ay/anaconda3/envs/mvcl/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ay/anaconda3/envs/mvcl/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.\n",
      "/home/ay/anaconda3/envs/mvcl/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (34) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 34/34 [00:03<00:00,  8.89it/s, v_num=0, val-clip_loss=2.200, val-mse_loss=1.000, val-cls_loss1D=0.688, val-cls_loss2D=0.692, val-cls_loss=0.689, val-contrast_loss=0.310, val-contrast_loss1D=0.330, val-contrast_loss2D=0.284, val-loss=4.880, val-acc=0.440, val-auc=0.458, val-eer=0.545, train-clip_loss=2.250, train-mse_loss=1.000, train-cls_loss1D=0.690, train-cls_loss2D=0.688, train-cls_loss=0.686, train-contrast_loss=0.296, train-contrast_loss1D=0.336, train-contrast_loss2D=0.276, train-loss=4.930, train-acc=0.440, train-auc=0.524, train-eer=0.536]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=4` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 34/34 [00:09<00:00,  3.73it/s, v_num=0, val-clip_loss=2.200, val-mse_loss=1.000, val-cls_loss1D=0.688, val-cls_loss2D=0.692, val-cls_loss=0.689, val-contrast_loss=0.310, val-contrast_loss1D=0.330, val-contrast_loss2D=0.284, val-loss=4.880, val-acc=0.440, val-auc=0.458, val-eer=0.545, train-clip_loss=2.250, train-mse_loss=1.000, train-cls_loss1D=0.690, train-cls_loss2D=0.688, train-cls_loss=0.686, train-contrast_loss=0.296, train-contrast_loss1D=0.336, train-contrast_loss2D=0.276, train-loss=4.930, train-acc=0.440, train-auc=0.524, train-eer=0.536]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(mvcl_lit, train_dataloader, val_dataloaders=val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, you can view the logging loss in the logger file, for example `logs/lightning_logs/version_0/metrics.csv`.\n",
    "![](imgs/loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After testing, the results will also saved in logger file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "/home/ay/anaconda3/envs/mvcl/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00<00:00, 29.50it/s]\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "       Test metric             DataLoader 0\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "        test-acc            0.44999998807907104\n",
      "        test-auc            0.4545454978942871\n",
      "     test-clip_loss         2.1567835807800293\n",
      "      test-cls_loss         0.6900644302368164\n",
      "     test-cls_loss1D        0.6895066499710083\n",
      "     test-cls_loss2D        0.6917417049407959\n",
      "   test-contrast_loss       0.31804102659225464\n",
      "  test-contrast_loss1D      0.34499993920326233\n",
      "  test-contrast_loss2D      0.28468912839889526\n",
      "        test-eer            0.5454545617103577\n",
      "        test-loss            4.857785701751709\n",
      "      test-mse_loss         1.0041100978851318\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test-clip_loss': 2.1567835807800293,\n",
       "  'test-mse_loss': 1.0041100978851318,\n",
       "  'test-cls_loss1D': 0.6895066499710083,\n",
       "  'test-cls_loss2D': 0.6917417049407959,\n",
       "  'test-cls_loss': 0.6900644302368164,\n",
       "  'test-contrast_loss': 0.31804102659225464,\n",
       "  'test-contrast_loss1D': 0.34499993920326233,\n",
       "  'test-contrast_loss2D': 0.28468912839889526,\n",
       "  'test-loss': 4.857785701751709,\n",
       "  'test-acc': 0.44999998807907104,\n",
       "  'test-auc': 0.4545454978942871,\n",
       "  'test-eer': 0.5454545617103577}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(mvcl_lit, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "Note, train, val, and test process will logging in the same file: `metrics.csv`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
