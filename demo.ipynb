{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from argparse import  Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#torch.__version__\n",
    "# !nvidia-smi\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build MVCL model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramFiles\\anaconda3\\envs\\my_project\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from MVCL import MultiViewModel_lit, MultiViewModel\n",
    "# pip install -r requirements.txt --cache-dir D:\\learn\\python-lib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Configuration Parameters\n",
    "\n",
    "| Parameter | Type | Default | Description |\n",
    "|-----------|------|---------|-------------|\n",
    "| `use_inner_CL` | int | 1 | Enable **inner contrastive learning** within the same modality for feature discrimination |\n",
    "| `use_inter_CL` | int | 1 | Enable **inter-modal contrastive learning** between different modalities |\n",
    "| `use_cls_loss_1_2` | int | 1 | Enable classification loss for modality 1 and modality 2 tasks |\n",
    "| `use_fusion` | int | 1 | Enable **feature fusion mechanism** to combine multi-modal features |\n",
    "| `use_fusion1D` | int | 1 | Enable **1D fusion** strategy for processing sequential feature fusion |\n",
    "| `use_fusion2D` | int | 1 | Enable **2D fusion** strategy for processing spatial feature map fusion |\n",
    "| `use_mse_loss` | int | 0 | Enable Mean Squared Error loss for regression tasks |\n",
    "| `only_1D` | int | 0 | **Use only 1D modality**, ignoring other dimensional features |\n",
    "| `only_2D` | int | 0 | **Use only 2D modality**, ignoring other dimensional features |\n",
    "| `drop_layer` | float | 0.0 | Dropout rate for regularization to prevent overfitting |\n",
    "| `w_con` | float | 1.0 | Weight coefficient for contrastive learning loss in total loss |\n",
    "| `w_cls` | float | 1.0 | Weight coefficient for classification loss in total loss |\n",
    "\n",
    "### Parameter Categories\n",
    "\n",
    "#### ğŸ¯ **Loss Function Control**\n",
    "- `use_inner_CL`, `use_inter_CL`: Control different types of contrastive learning\n",
    "- `use_cls_loss_1_2`: Control classification loss\n",
    "- `use_mse_loss`: Control regression loss\n",
    "\n",
    "#### ğŸ”„ **Feature Fusion Strategy**\n",
    "- `use_fusion`: Master switch for feature fusion\n",
    "- `use_fusion1D`, `use_fusion2D`: Control fusion methods for different dimensions\n",
    "\n",
    "#### ğŸ›ï¸ **Modality Selection**\n",
    "- `only_1D`, `only_2D`: Control whether to use only specific dimensional modalities\n",
    "\n",
    "#### âš–ï¸ **Weight Balancing**\n",
    "- `w_con`, `w_cls`: Balance the importance of different loss functions\n",
    "- `drop_layer`: Regularization parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### default model configuration\n",
    "mvcl_cfg = Namespace(\n",
    "    use_inner_CL=1,\n",
    "    use_inter_CL=1,\n",
    "    use_cls_loss_1_2=1,\n",
    "    use_fusion=1,\n",
    "    use_fusion1D=1,\n",
    "    use_fusion2D=1,\n",
    "    use_mse_loss=0,\n",
    "    only_1D=0,\n",
    "    only_2D=0,\n",
    "    drop_layer=0.0,\n",
    "    w_con=1.0,\n",
    "    w_cls=1.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first building, this will download the Wav2Clip model checkpoints and the WavLM model checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "mvcl = MultiViewModel(cfg=mvcl_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input of our MVCL model is a torch Tensor with shape of (batch, 1, audio_length).\n",
    "\n",
    "Take an random tensor as example. The batch size is 2, indicating this output is from processing 2 audio samples simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(2, 1, 48000)\n",
    "res = mvcl(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw_spec: torch.Size([2, 1, 257, 257])\n",
      "raw_wav_feat: torch.Size([2, 149, 768])\n",
      "feature1D: torch.Size([2, 768])\n",
      "feature2D: torch.Size([2, 512])\n",
      "feature: torch.Size([2, 1280])\n",
      "logit1D: torch.Size([2])\n",
      "logit2D: torch.Size([2])\n",
      "logit: torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "for k, v in res.items():\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        print(f\"{k}: {v.shape}\")\n",
    "    else:\n",
    "        print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is a dict:\n",
    "\n",
    "| Feature Name | Shape | Dimension | Description |\n",
    "|--------------|-------|-----------|-------------|\n",
    "| `raw_spec` | `[2, 1, 257, 257]` | 4D | **Raw spectrogram** - Original frequency-time representation of audio signal with 257 frequency bins and 257 time frames |\n",
    "| `raw_wav_feat` | `[2, 149, 768]` | 3D | **Raw waveform features** - Sequential audio features extracted from backbone (e.g., WavLM), 149 time steps with 768-dimensional embeddings |\n",
    "| `feature1D` | `[2, 768]` | 2D | **1D modality features** - the final classification feat of the 1D branch |\n",
    "| `feature2D` | `[2, 512]` | 2D | **2D modality features** - the final classification feat of the 2D branch |\n",
    "| `feature` | `[2, 1280]` | 2D | **Fused features** - Combined multi-modal features (1D + 2D), concatenated to 1280 dimensions (768 + 512) |\n",
    "| `logit1D` | `[2]` | 1D | **1D modality logits** - Classification scores from 1D feature branch for binary classification |\n",
    "| `logit2D` | `[2]` | 1D | **2D modality logits** - Classification scores from 2D feature branch for binary classification |\n",
    "| `logit` | `[2]` | 1D | **Final logits** - Combined classification scores from fused features for final prediction |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Feature Processing Pipeline**\n",
    "\n",
    "```\n",
    "Audio Input (batch, 1, 48000)\n",
    "    â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    Stage 1 (No Grad)                       â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚      1D Branch           â”‚           2D Branch              â”‚\n",
    "â”‚                          â”‚                                  â”‚\n",
    "â”‚ feature_model1D          â”‚ feature_model2D                  â”‚\n",
    "â”‚ .compute_stage1(x)       â”‚ .compute_stage1(x, spec_aug)     â”‚\n",
    "â”‚      â†“                   â”‚      â†“                           â”‚\n",
    "â”‚    wav1                  â”‚   spec1, raw_spec               â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                           â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                 Cross-Modal Fusion                          â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚   squeeze_modules[0]     â”‚     expand_modules[0]            â”‚\n",
    "â”‚   (wav1, spec1)          â”‚     (wav1, spec1)                â”‚\n",
    "â”‚      â†“                   â”‚      â†“                           â”‚\n",
    "â”‚   fused_wav1             â”‚   fused_spec1                    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                           â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    Stage 2                                  â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ feature_model1D          â”‚ feature_model2D                  â”‚\n",
    "â”‚ .compute_stage2          â”‚ .compute_stage2                  â”‚\n",
    "â”‚ (fused_wav1)             â”‚ (fused_spec1)                    â”‚\n",
    "â”‚      â†“                   â”‚      â†“                           â”‚\n",
    "â”‚ wav2, position_bias      â”‚   spec2                          â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                           â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚              Cross-Modal Fusion + Stage 3                   â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚   squeeze_modules[1]     â”‚     expand_modules[1]            â”‚\n",
    "â”‚   (wav2, spec2)          â”‚     (wav2, spec2)                â”‚\n",
    "â”‚      â†“                   â”‚      â†“                           â”‚\n",
    "â”‚ feature_model1D          â”‚ feature_model2D                  â”‚\n",
    "â”‚ .compute_stage3          â”‚ .compute_stage3                  â”‚\n",
    "â”‚      â†“                   â”‚      â†“                           â”‚\n",
    "â”‚ wav3, position_bias      â”‚   spec3                          â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                           â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚              Cross-Modal Fusion + Stage 4                   â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚   squeeze_modules[2]     â”‚     expand_modules[2]            â”‚\n",
    "â”‚   (wav3, spec3)          â”‚     (wav3, spec3)                â”‚\n",
    "â”‚      â†“                   â”‚      â†“                           â”‚\n",
    "â”‚ feature_model1D          â”‚ feature_model2D                  â”‚\n",
    "â”‚ .compute_stage4          â”‚ .compute_stage4                  â”‚\n",
    "â”‚      â†“                   â”‚      â†“                           â”‚\n",
    "â”‚ wav4, position_bias      â”‚   spec4                          â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                           â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚           Final Cross-Modal Fusion + Latent Features        â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚   squeeze_modules[3]     â”‚     expand_modules[3]            â”‚\n",
    "â”‚   (wav4, spec4)          â”‚     (wav4, spec4)                â”‚\n",
    "â”‚      â†“                   â”‚      â†“                           â”‚\n",
    "â”‚ feature_model1D          â”‚ feature_model2D                  â”‚\n",
    "â”‚ .compute_latent_feature  â”‚ .compute_latent_feature          â”‚\n",
    "â”‚      â†“                   â”‚      â†“                           â”‚\n",
    "â”‚ wav5, raw_wav_feat       â”‚   spec5                          â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                           â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                Feature Normalization                        â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚   norm_feat(wav5)        â”‚   norm_feat(spec5)               â”‚\n",
    "â”‚      â†“                   â”‚      â†“                           â”‚\n",
    "â”‚   feature1D [B, 768]     â”‚   feature2D [B, 512]             â”‚\n",
    "â”‚      â†“                   â”‚      â†“                           â”‚\n",
    "â”‚   cls1D(feature1D)       â”‚   cls2D(feature2D)               â”‚\n",
    "â”‚      â†“                   â”‚      â†“                           â”‚\n",
    "â”‚   logit1D [B]            â”‚   logit2D [B]                    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                           â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                   Multi-Modal Fusion                        â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚        concat([wav5, spec5], dim=-1)                        â”‚\n",
    "â”‚                     â†“                                       â”‚\n",
    "â”‚              norm_feat(concat)                              â”‚\n",
    "â”‚                     â†“                                       â”‚\n",
    "â”‚               feature [B, 1280]                             â”‚\n",
    "â”‚                     â†“                                       â”‚\n",
    "â”‚              cls_final(feature)                             â”‚\n",
    "â”‚                     â†“                                       â”‚\n",
    "â”‚                logit [B]                                    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lit model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> PyTorch Lightning is the deep learning framework for professional AI researchers and machine learning engineers who need maximal flexibility without sacrificing performance at scale. Lightning evolves with you as your projects go from idea to paper/production\n",
    "\n",
    "We use [pytorch_lightning](https://lightning.ai/docs/pytorch/stable/) to train, validate, and test our model. Besides, it can also easily control the logging, model saving and callbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import Trainer, LightningModule\n",
    "from pytorch_lightning.loggers import  CSVLogger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the pytorch Lightning module to train the model, where we define the train step, validation/predict step, loss function and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BCE loss with label smoothing:  0.1\n"
     ]
    }
   ],
   "source": [
    "mvcl_lit = MultiViewModel_lit(cfg=mvcl_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test forwarding "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lit model, we use the `_shared_pred` method to predict the logits of the input batch. If the stage is train, we also the the audio_transform to augment the spectrogram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a random batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = {\n",
    "    \"label\": torch.randint(0, 2, (3,)),\n",
    "    \"audio\": torch.randn(3, 1, 48000),\n",
    "    \"sample_rate\": [16000, 16000, 16000],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, **your batch must be a dict with above keys**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, the `_shared_pred` output is also a dict. We use it to compute the loss\n",
    "function, AUC, and ERR scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw_spec torch.Size([3, 1, 257, 257])\n",
      "raw_wav_feat torch.Size([3, 149, 768])\n",
      "feature1D torch.Size([3, 768])\n",
      "feature2D torch.Size([3, 512])\n",
      "feature torch.Size([3, 1280])\n",
      "logit1D torch.Size([3])\n",
      "logit2D torch.Size([3])\n",
      "logit torch.Size([3])\n",
      "pred torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "batch_res = mvcl_lit._shared_pred(batch=batch, batch_idx=0)\n",
    "for key, value in batch_res.items():\n",
    "    print(key, value.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first build a simple dataloaders for training, where all the samples are randomly generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from callbacks import EER_Callback, BinaryAUC_Callback, BinaryACC_Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a dataloader with random values for demo training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£åœ¨å¤„ç†æµ‹è¯•é›†åè®®: D:\\learn\\MVCL-ADD\\download\\archive\\LA\\ASVspoof2019_LA_cm_protocols\\ASVspoof2019.LA.cm.eval.trl.txt\n",
      "âœ… ç”Ÿæˆå®Œæ¯•ï¼å…± 71237 æ¡æ•°æ®ã€‚\n",
      "ğŸ“„ åˆ—è¡¨æ–‡ä»¶ä¿å­˜åœ¨: data_lists\\eval_list.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# === 1. é…ç½®å®é™…è·¯å¾„ ===\n",
    "# download/archive/LA\n",
    "DATA_ROOT = r\"D:\\learn\\MVCL-ADD\\download\\archive\\LA\"\n",
    "\n",
    "# å®šä¹‰è¾“å‡ºåˆ—è¡¨æ–‡ä»¶çš„ä½ç½®\n",
    "LIST_SAVE_DIR = \"data_lists\"\n",
    "if not os.path.exists(LIST_SAVE_DIR):\n",
    "    os.makedirs(LIST_SAVE_DIR)\n",
    "\n",
    "def generate_eval_list():\n",
    "    # åè®®æ–‡ä»¶è·¯å¾„ (CM protocols)\n",
    "    protocol_path = os.path.join(\n",
    "        DATA_ROOT, \n",
    "        \"ASVspoof2019_LA_cm_protocols\", \n",
    "        \"ASVspoof2019.LA.cm.eval.trl.txt\"\n",
    "    )\n",
    "    \n",
    "    # éŸ³é¢‘æ–‡ä»¶å¤¹è·¯å¾„\n",
    "    audio_dir = os.path.join(DATA_ROOT, \"ASVspoof2019_LA_eval\", \"flac\")\n",
    "    \n",
    "    output_path = os.path.join(LIST_SAVE_DIR, \"eval_list.txt\")\n",
    "    \n",
    "    if not os.path.exists(protocol_path):\n",
    "        print(f\"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°åè®®æ–‡ä»¶: {protocol_path}\")\n",
    "        return None\n",
    "        \n",
    "    print(f\"æ­£åœ¨å¤„ç†æµ‹è¯•é›†åè®®: {protocol_path}\")\n",
    "    \n",
    "    count = 0\n",
    "    with open(protocol_path, 'r') as f_in, open(output_path, 'w') as f_out:\n",
    "        for line in f_in:\n",
    "            parts = line.strip().split(' ')\n",
    "            # æ ¼å¼: LA_0069 LA_E_1000147 - - spoof\n",
    "            audio_name = parts[1]\n",
    "            label_str = parts[4]\n",
    "            \n",
    "            # è½¬æ¢æ ‡ç­¾: bonafide(çœŸ)=1, spoof(å‡)=0\n",
    "            label = 1 if label_str == 'bonafide' else 0\n",
    "            \n",
    "            # æ‹¼æ¥å®Œæ•´ç»å¯¹è·¯å¾„\n",
    "            full_path = os.path.join(audio_dir, audio_name + '.flac')\n",
    "            \n",
    "            # å†™å…¥ä¸€è¡Œ: è·¯å¾„ æ ‡ç­¾\n",
    "            f_out.write(f\"{full_path} {label}\\n\")\n",
    "            count += 1\n",
    "            \n",
    "    print(f\"âœ… ç”Ÿæˆå®Œæ¯•ï¼å…± {count} æ¡æ•°æ®ã€‚\")\n",
    "    print(f\"ğŸ“„ åˆ—è¡¨æ–‡ä»¶ä¿å­˜åœ¨: {output_path}\")\n",
    "    return output_path\n",
    "\n",
    "# æ‰§è¡Œç”Ÿæˆ\n",
    "eval_list_path = generate_eval_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… soundfile åº“å·²æ£€æµ‹åˆ° (æˆ‘ä»¬å°†å¼ºåˆ¶ä½¿ç”¨å®ƒ)\n",
      "ğŸ“¦ Dataset åŠ è½½å®Œæˆï¼Œå…± 71237 ä¸ªæ ·æœ¬\n",
      "ğŸš€ DataLoader å‡†å¤‡å°±ç»ªï¼\n",
      "   æµ‹è¯•é›†æ ·æœ¬æ•°: 71237\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# === å¼ºåˆ¶æ£€æŸ¥ soundfile æ˜¯å¦å®‰è£… ===\n",
    "try:\n",
    "    import soundfile\n",
    "    print(\"âœ… soundfile åº“å·²æ£€æµ‹åˆ° (æˆ‘ä»¬å°†å¼ºåˆ¶ä½¿ç”¨å®ƒ)\")\n",
    "except ImportError:\n",
    "    # å¦‚æœè¿™é‡ŒæŠ¥é”™ï¼Œè¯·åœ¨ä¸€ä¸ªæ–° Cell é‡Œè¿è¡Œ: !pip install soundfile\n",
    "    raise ImportError(\"âŒ ä¸¥é‡é”™è¯¯: è¯·å…ˆå®‰è£…soundfile\")\n",
    "\n",
    "\n",
    "# ===  å®šä¹‰ Dataset ===\n",
    "class ASVspoofDataset(Dataset):\n",
    "    def __init__(self, list_path, max_len=64000):\n",
    "        \"\"\"\n",
    "        list_path: ç¬¬ä¸€æ­¥ç”Ÿæˆçš„ txt æ–‡ä»¶è·¯å¾„\n",
    "        max_len: éŸ³é¢‘é•¿åº¦é™åˆ¶ (64000ç‚¹ â‰ˆ 4ç§’ï¼ŒWavLMé€šå¸¸éœ€è¦å®šé•¿è¾“å…¥)\n",
    "        \"\"\"\n",
    "        self.data_list = []\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        # è¯»å–åˆ—è¡¨\n",
    "        with open(list_path, 'r') as f:\n",
    "            for line in f:\n",
    "                path, label = line.strip().split(' ')\n",
    "                self.data_list.append((path, int(label)))\n",
    "        \n",
    "        print(f\"ğŸ“¦ Dataset åŠ è½½å®Œæˆï¼Œå…± {len(self.data_list)} ä¸ªæ ·æœ¬\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_path, label = self.data_list[idx]\n",
    "        \n",
    "        # 1. åŠ è½½éŸ³é¢‘ (torchaudioä¼šè‡ªåŠ¨å¤„ç†flac)\n",
    "        # waveform shape: [Channels, Time]ï¼Œ æŒ‡å®šbackendåç«¯å¼•æ“æ¥è¯»å–éŸ³é¢‘æ–‡ä»¶\n",
    "        waveform, sample_rate = torchaudio.load(audio_path, backend=\"soundfile\")\n",
    "        \n",
    "        # 2. ç»Ÿä¸€é•¿åº¦å¤„ç† (Pad or Crop)\n",
    "        curr_len = waveform.shape[1]\n",
    "        if curr_len < self.max_len:\n",
    "            # å¦‚æœçŸ­äº†ï¼Œåœ¨å³è¾¹è¡¥é›¶\n",
    "            pad_width = self.max_len - curr_len\n",
    "            waveform = torch.nn.functional.pad(waveform, (0, pad_width))\n",
    "        else:\n",
    "            # å¦‚æœé•¿äº†ï¼Œæˆªå–å‰ max_len\n",
    "            waveform = waveform[:, :self.max_len]\n",
    "            \n",
    "        # 3. è¿”å›å­—å…¸ (ç¬¦åˆ mvcl_lit çš„è¾“å…¥è¦æ±‚)\n",
    "        return {\n",
    "            \"audio\": waveform,       # [1, 64000]\n",
    "            \"label\": label,          # 0 æˆ– 1\n",
    "            \"sample_rate\": sample_rate\n",
    "        }\n",
    "    \n",
    "    \n",
    "# 1. å®šä¹‰åˆ—è¡¨æ–‡ä»¶è·¯å¾„ (ç¡®ä¿è¿™ä¸ªæ–‡ä»¶å­˜åœ¨ï¼)\n",
    "eval_list_path = \"data_lists/eval_list.txt\" \n",
    "\n",
    "# 2. åˆ›å»ºçœŸå®æ•°æ®çš„ Dataset å’Œ DataLoader\n",
    "# æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬åªåˆ›å»º test_dataloaderï¼Œå› ä¸ºä½ ç›®å‰åªç”Ÿæˆäº† eval çš„åˆ—è¡¨\n",
    "test_dataset = ASVspoofDataset(list_path=eval_list_path, max_len=64000)\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=16,      # æ ¹æ®æ˜¾å­˜è°ƒæ•´ï¼Œæ¨è 8 æˆ– 16\n",
    "    shuffle=False,      # æµ‹è¯•é›†ä¸éœ€è¦æ‰“ä¹±\n",
    "    num_workers=0       # Windows ä¸‹å¿…é¡»è®¾ä¸º 0\n",
    ")\n",
    "\n",
    "print(f\"ğŸš€ DataLoader å‡†å¤‡å°±ç»ªï¼\")\n",
    "print(f\"   æµ‹è¯•é›†æ ·æœ¬æ•°: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "ğŸš€ å¼€å§‹å¯¹ 71237 æ¡çœŸå®æ•°æ®è¿›è¡Œæ¨ç†...\n",
      "    (ç”±äºæ•°æ®é‡è¾ƒå¤§ï¼Œè¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…è¿›åº¦æ¡èµ°å®Œ)\n",
      "------------------------------\n",
      "Testing: |          | 0/? [00:27<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "TorchCodec is required for load_with_torchcodec. Please install torchcodec to use this function.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ProgramFiles\\anaconda3\\envs\\my_project\\Lib\\site-packages\\torchaudio\\_torchcodec.py:82\u001b[39m, in \u001b[36mload_with_torchcodec\u001b[39m\u001b[34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size, backend)\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchcodec\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdecoders\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AudioDecoder\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torchcodec'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m30\u001b[39m)\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# æ­£å¼æµ‹è¯•\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmvcl_lit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ProgramFiles\\anaconda3\\envs\\my_project\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:775\u001b[39m, in \u001b[36mTrainer.test\u001b[39m\u001b[34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[39m\n\u001b[32m    773\u001b[39m \u001b[38;5;28mself\u001b[39m.state.status = TrainerStatus.RUNNING\n\u001b[32m    774\u001b[39m \u001b[38;5;28mself\u001b[39m.testing = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m775\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    776\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_test_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\n\u001b[32m    777\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ProgramFiles\\anaconda3\\envs\\my_project\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:48\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     46\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m trainer.strategy.launcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     47\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[32m     51\u001b[39m     _call_teardown_hook(trainer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ProgramFiles\\anaconda3\\envs\\my_project\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:817\u001b[39m, in \u001b[36mTrainer._test_impl\u001b[39m\u001b[34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[39m\n\u001b[32m    813\u001b[39m     download_model_from_registry(ckpt_path, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m    814\u001b[39m ckpt_path = \u001b[38;5;28mself\u001b[39m._checkpoint_connector._select_ckpt_path(\n\u001b[32m    815\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.fn, ckpt_path, model_provided=model_provided, model_connected=\u001b[38;5;28mself\u001b[39m.lightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    816\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m817\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    818\u001b[39m \u001b[38;5;66;03m# remove the tensors from the test results\u001b[39;00m\n\u001b[32m    819\u001b[39m results = convert_tensors_to_scalars(results)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ProgramFiles\\anaconda3\\envs\\my_project\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1012\u001b[39m, in \u001b[36mTrainer._run\u001b[39m\u001b[34m(self, model, ckpt_path)\u001b[39m\n\u001b[32m   1007\u001b[39m \u001b[38;5;28mself\u001b[39m._signal_connector.register_signal_handlers()\n\u001b[32m   1009\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1010\u001b[39m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[32m   1011\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1012\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1015\u001b[39m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[32m   1016\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1017\u001b[39m log.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: trainer tearing down\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ProgramFiles\\anaconda3\\envs\\my_project\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1049\u001b[39m, in \u001b[36mTrainer._run_stage\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1046\u001b[39m \u001b[38;5;28mself\u001b[39m.lightning_module.zero_grad()\n\u001b[32m   1048\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.evaluating:\n\u001b[32m-> \u001b[39m\u001b[32m1049\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_evaluation_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1050\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.predicting:\n\u001b[32m   1051\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.predict_loop.run()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ProgramFiles\\anaconda3\\envs\\my_project\\Lib\\site-packages\\pytorch_lightning\\loops\\utilities.py:179\u001b[39m, in \u001b[36m_no_grad_context.<locals>._decorator\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    177\u001b[39m     context_manager = torch.no_grad\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ProgramFiles\\anaconda3\\envs\\my_project\\Lib\\site-packages\\pytorch_lightning\\loops\\evaluation_loop.py:138\u001b[39m, in \u001b[36m_EvaluationLoop.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    137\u001b[39m     dataloader_iter = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m     batch, batch_idx, dataloader_idx = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m previous_dataloader_idx != dataloader_idx:\n\u001b[32m    140\u001b[39m     \u001b[38;5;66;03m# the dataloader has changed, notify the logger connector\u001b[39;00m\n\u001b[32m    141\u001b[39m     \u001b[38;5;28mself\u001b[39m._store_dataloader_outputs()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ProgramFiles\\anaconda3\\envs\\my_project\\Lib\\site-packages\\pytorch_lightning\\loops\\fetchers.py:134\u001b[39m, in \u001b[36m_PrefetchDataFetcher.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    131\u001b[39m         \u001b[38;5;28mself\u001b[39m.done = \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.batches\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.done:\n\u001b[32m    133\u001b[39m     \u001b[38;5;66;03m# this will run only when no pre-fetching was done.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     batch = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__next__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    136\u001b[39m     \u001b[38;5;66;03m# the iterator is empty\u001b[39;00m\n\u001b[32m    137\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ProgramFiles\\anaconda3\\envs\\my_project\\Lib\\site-packages\\pytorch_lightning\\loops\\fetchers.py:61\u001b[39m, in \u001b[36m_DataFetcher.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     59\u001b[39m \u001b[38;5;28mself\u001b[39m._start_profiler()\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m     batch = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m     63\u001b[39m     \u001b[38;5;28mself\u001b[39m.done = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ProgramFiles\\anaconda3\\envs\\my_project\\Lib\\site-packages\\pytorch_lightning\\utilities\\combined_loader.py:341\u001b[39m, in \u001b[36mCombinedLoader.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    339\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> _ITERATOR_RETURN:\n\u001b[32m    340\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m341\u001b[39m     out = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    342\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m._iterator, _Sequential):\n\u001b[32m    343\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ProgramFiles\\anaconda3\\envs\\my_project\\Lib\\site-packages\\pytorch_lightning\\utilities\\combined_loader.py:142\u001b[39m, in \u001b[36m_Sequential.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    139\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     out = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miterators\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    144\u001b[39m     \u001b[38;5;66;03m# try the next iterator\u001b[39;00m\n\u001b[32m    145\u001b[39m     \u001b[38;5;28mself\u001b[39m._use_next_iterator()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ProgramFiles\\anaconda3\\envs\\my_project\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:741\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    738\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    739\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    740\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m741\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    742\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    743\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    744\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    745\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    746\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    747\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ProgramFiles\\anaconda3\\envs\\my_project\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:801\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    799\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    800\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m801\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    802\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    803\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ProgramFiles\\anaconda3\\envs\\my_project\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     52\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     53\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m         data = \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     56\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ProgramFiles\\anaconda3\\envs\\my_project\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     52\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     53\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     56\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 40\u001b[39m, in \u001b[36mASVspoofDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     36\u001b[39m audio_path, label = \u001b[38;5;28mself\u001b[39m.data_list[idx]\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# 1. åŠ è½½éŸ³é¢‘ (torchaudioä¼šè‡ªåŠ¨å¤„ç†flac)\u001b[39;00m\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# waveform shape: [Channels, Time]ï¼Œ æŒ‡å®šbackendåç«¯å¼•æ“æ¥è¯»å–éŸ³é¢‘æ–‡ä»¶\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m waveform, sample_rate = \u001b[43mtorchaudio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msoundfile\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# 2. ç»Ÿä¸€é•¿åº¦å¤„ç† (Pad or Crop)\u001b[39;00m\n\u001b[32m     43\u001b[39m curr_len = waveform.shape[\u001b[32m1\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ProgramFiles\\anaconda3\\envs\\my_project\\Lib\\site-packages\\torchaudio\\__init__.py:86\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size, backend)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\n\u001b[32m     19\u001b[39m     uri: Union[BinaryIO, \u001b[38;5;28mstr\u001b[39m, os.PathLike],\n\u001b[32m     20\u001b[39m     frame_offset: \u001b[38;5;28mint\u001b[39m = \u001b[32m0\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     26\u001b[39m     backend: Optional[\u001b[38;5;28mstr\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     27\u001b[39m ) -> Tuple[torch.Tensor, \u001b[38;5;28mint\u001b[39m]:\n\u001b[32m     28\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load audio data from source using TorchCodec's AudioDecoder.\u001b[39;00m\n\u001b[32m     29\u001b[39m \n\u001b[32m     30\u001b[39m \u001b[33;03m    .. note::\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     84\u001b[39m \u001b[33;03m        by TorchCodec.\u001b[39;00m\n\u001b[32m     85\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_with_torchcodec\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[43m        \u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[43m        \u001b[49m\u001b[43mframe_offset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mframe_offset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchannels_first\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchannels_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ProgramFiles\\anaconda3\\envs\\my_project\\Lib\\site-packages\\torchaudio\\_torchcodec.py:84\u001b[39m, in \u001b[36mload_with_torchcodec\u001b[39m\u001b[34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size, backend)\u001b[39m\n\u001b[32m     82\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchcodec\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdecoders\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AudioDecoder\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     85\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mTorchCodec is required for load_with_torchcodec. \u001b[39m\u001b[33m\"\u001b[39m \u001b[33m\"\u001b[39m\u001b[33mPlease install torchcodec to use this function.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     86\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m     88\u001b[39m \u001b[38;5;66;03m# Parameter validation and warnings\u001b[39;00m\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m normalize:\n",
      "\u001b[31mImportError\u001b[39m: TorchCodec is required for load_with_torchcodec. Please install torchcodec to use this function."
     ]
    }
   ],
   "source": [
    "# === æœ€ç»ˆæµ‹è¯•æ‰§è¡Œè„šæœ¬ ===\n",
    "\n",
    "# 1. å®‰å…¨æ£€æŸ¥ï¼šå¦‚æœæ¨¡å‹æˆ– Trainer ä¸¢å¤±äº†ï¼Œè‡ªåŠ¨é‡æ–°åˆå§‹åŒ–\n",
    "if 'trainer' not in locals() or 'mvcl_lit' not in locals():\n",
    "    print(\"âš ï¸ æ£€æµ‹åˆ°ç¯å¢ƒè¢«é‡ç½®ï¼Œæ­£åœ¨é‡æ–°åˆå§‹åŒ–æ¨¡å‹å’Œ Trainer...\")\n",
    "    from pytorch_lightning import Trainer\n",
    "    from pytorch_lightning.loggers import CSVLogger\n",
    "    from MVCL import MultiViewModel_lit\n",
    "    from argparse import Namespace\n",
    "    \n",
    "    # é…ç½®\n",
    "    mvcl_cfg = Namespace(\n",
    "        use_inner_CL=1, use_inter_CL=1, use_cls_loss_1_2=1,\n",
    "        use_fusion=1, use_fusion1D=1, use_fusion2D=1,\n",
    "        use_mse_loss=0, only_1D=0, only_2D=0,\n",
    "        drop_layer=0.0, w_con=1.0, w_cls=1.0,\n",
    "    )\n",
    "    # æ¨¡å‹\n",
    "    mvcl_lit = MultiViewModel_lit(cfg=mvcl_cfg)\n",
    "    # Trainer\n",
    "    trainer = Trainer(\n",
    "        accelerator=\"gpu\", \n",
    "        devices=[0], \n",
    "        enable_checkpointing=False, \n",
    "        logger=CSVLogger(save_dir=\"./logs\", version=99) # ä¸´æ—¶æ—¥å¿—\n",
    "    )\n",
    "    print(\"âœ… åˆå§‹åŒ–å®Œæˆï¼\")\n",
    "\n",
    "# 2. è¿è¡Œæµ‹è¯•\n",
    "print(\"-\" * 30)\n",
    "print(f\"ğŸš€ å¼€å§‹å¯¹ {len(test_dataloader.dataset)} æ¡çœŸå®æ•°æ®è¿›è¡Œæ¨ç†...\")\n",
    "print(\"    (ç”±äºæ•°æ®é‡è¾ƒå¤§ï¼Œè¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…è¿›åº¦æ¡èµ°å®Œ)\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# æ­£å¼æµ‹è¯•\n",
    "trainer.test(mvcl_lit, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTestDataset(Dataset):\n",
    "    def __init__(self, num_samples=10):\n",
    "        # Generate synthetic data similar to your example\n",
    "        self.samples = []\n",
    "        for _ in range(num_samples):\n",
    "            self.samples.append({\n",
    "                \"audio\": torch.randn(1, 48000),\n",
    "                \"label\": torch.randint(0, 2, (1,)).item(),\n",
    "                \"sample_rate\": 16000,\n",
    "            })\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n",
    "\n",
    "\n",
    "\n",
    "# Create the dataset and dataloader\n",
    "train_dataloader = DataLoader(\n",
    "    SimpleTestDataset(num_samples=100),\n",
    "    batch_size=3,\n",
    "    shuffle=True,\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    SimpleTestDataset(num_samples=50),\n",
    "    batch_size=3,\n",
    "    shuffle=False,\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    SimpleTestDataset(num_samples=20),\n",
    "    batch_size=3,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build a simple trainer to train and test our model, which uses:\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    logger=CSVLogger(save_dir=\"./logs\", version=0),\n",
    "    max_epochs=4,\n",
    "    callbacks=[\n",
    "        BinaryACC_Callback(batch_key=\"label\", output_key=\"logit\"),\n",
    "        BinaryAUC_Callback(batch_key=\"label\", output_key=\"logit\"),\n",
    "        EER_Callback(batch_key=\"label\", output_key=\"logit\"),\n",
    "    ],\n",
    "    devices=[0], # use cuda:0 device\n",
    "    accelerator=\"gpu\", # use GPU acceleration\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 5070 Ti Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "d:\\ProgramFiles\\anaconda3\\envs\\my_project\\Lib\\site-packages\\lightning_fabric\\loggers\\csv_logs.py:268: Experiment logs directory ./logs\\lightning_logs\\version_0 exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!\n",
      "d:\\ProgramFiles\\anaconda3\\envs\\my_project\\Lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:658: Checkpoint directory ./logs\\lightning_logs\\version_0\\checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                | Type                            | Params | Mode \n",
      "--------------------------------------------------------------------------------\n",
      "0 | model               | MultiViewModel                  | 128 M  | train\n",
      "1 | clip_heads          | ModuleList                      | 1.6 M  | train\n",
      "2 | bce_loss            | LabelSmoothingBCE               | 0      | train\n",
      "3 | contrast_loss2      | BinaryTokenContrastLoss         | 0      | train\n",
      "4 | triplet_loss        | TripletMarginLoss               | 0      | train\n",
      "5 | clip_loss           | CLIPLoss1D                      | 1      | train\n",
      "6 | reconstruction_loss | TimeFrequencyReconstructionLoss | 379 K  | train\n",
      "--------------------------------------------------------------------------------\n",
      "130 M     Trainable params\n",
      "0         Non-trainable params\n",
      "130 M     Total params\n",
      "521.061   Total estimated model params size (MB)\n",
      "210       Modules in train mode\n",
      "233       Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramFiles\\anaconda3\\envs\\my_project\\Lib\\site-packages\\pytorch_lightning\\utilities\\_pytree.py:21: `isinstance(treespec, LeafSpec)` is deprecated, use `isinstance(treespec, TreeSpec) and treespec.is_leaf()` instead.\n",
      "d:\\ProgramFiles\\anaconda3\\envs\\my_project\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramFiles\\anaconda3\\envs\\my_project\\Lib\\site-packages\\pytorch_lightning\\utilities\\_pytree.py:21: `isinstance(treespec, LeafSpec)` is deprecated, use `isinstance(treespec, TreeSpec) and treespec.is_leaf()` instead.\n",
      "d:\\ProgramFiles\\anaconda3\\envs\\my_project\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "d:\\ProgramFiles\\anaconda3\\envs\\my_project\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:310: The number of training batches (34) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34/34 [00:03<00:00,  8.69it/s, v_num=0, val-clip_loss=2.210, val-mse_loss=1.000, val-cls_loss1D=0.695, val-cls_loss2D=0.695, val-cls_loss=0.695, val-contrast_loss=0.310, val-contrast_loss1D=0.330, val-contrast_loss2D=0.289, val-loss=4.910, val-acc=0.540, val-auc=0.523, val-eer=0.522, train-clip_loss=2.240, train-mse_loss=1.000, train-cls_loss1D=0.694, train-cls_loss2D=0.692, train-cls_loss=0.691, train-contrast_loss=0.294, train-contrast_loss1D=0.324, train-contrast_loss2D=0.290, train-loss=4.930, train-acc=0.560, train-auc=0.591, train-eer=0.400]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=4` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34/34 [00:08<00:00,  3.94it/s, v_num=0, val-clip_loss=2.210, val-mse_loss=1.000, val-cls_loss1D=0.695, val-cls_loss2D=0.695, val-cls_loss=0.695, val-contrast_loss=0.310, val-contrast_loss1D=0.330, val-contrast_loss2D=0.289, val-loss=4.910, val-acc=0.540, val-auc=0.523, val-eer=0.522, train-clip_loss=2.240, train-mse_loss=1.000, train-cls_loss1D=0.694, train-cls_loss2D=0.692, train-cls_loss=0.691, train-contrast_loss=0.294, train-contrast_loss1D=0.324, train-contrast_loss2D=0.290, train-loss=4.930, train-acc=0.560, train-auc=0.591, train-eer=0.400]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(mvcl_lit, train_dataloader, val_dataloaders=val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, you can view the logging loss in the logger file, for example `logs/lightning_logs/version_0/metrics.csv`.\n",
    "![](imgs/loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After testing, the results will also saved in logger file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "d:\\ProgramFiles\\anaconda3\\envs\\my_project\\Lib\\site-packages\\pytorch_lightning\\utilities\\_pytree.py:21: `isinstance(treespec, LeafSpec)` is deprecated, use `isinstance(treespec, TreeSpec) and treespec.is_leaf()` instead.\n",
      "d:\\ProgramFiles\\anaconda3\\envs\\my_project\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00, 30.22it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\">        Test metric        </span>â”ƒ<span style=\"font-weight: bold\">       DataLoader 0        </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\">         test-acc          </span>â”‚<span style=\"color: #800080; text-decoration-color: #800080\">     0.550000011920929     </span>â”‚\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\">         test-auc          </span>â”‚<span style=\"color: #800080; text-decoration-color: #800080\">    0.43434345722198486    </span>â”‚\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\">      test-clip_loss       </span>â”‚<span style=\"color: #800080; text-decoration-color: #800080\">    2.1738295555114746     </span>â”‚\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\">       test-cls_loss       </span>â”‚<span style=\"color: #800080; text-decoration-color: #800080\">    0.6950899362564087     </span>â”‚\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\">      test-cls_loss1D      </span>â”‚<span style=\"color: #800080; text-decoration-color: #800080\">    0.6951161026954651     </span>â”‚\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\">      test-cls_loss2D      </span>â”‚<span style=\"color: #800080; text-decoration-color: #800080\">    0.6945802569389343     </span>â”‚\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\">    test-contrast_loss     </span>â”‚<span style=\"color: #800080; text-decoration-color: #800080\">    0.3196900486946106     </span>â”‚\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\">   test-contrast_loss1D    </span>â”‚<span style=\"color: #800080; text-decoration-color: #800080\">    0.35999998450279236    </span>â”‚\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\">   test-contrast_loss2D    </span>â”‚<span style=\"color: #800080; text-decoration-color: #800080\">    0.27993908524513245    </span>â”‚\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\">         test-eer          </span>â”‚<span style=\"color: #800080; text-decoration-color: #800080\">    0.6363636255264282     </span>â”‚\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\">         test-loss         </span>â”‚<span style=\"color: #800080; text-decoration-color: #800080\">     4.898554801940918     </span>â”‚\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\">       test-mse_loss       </span>â”‚<span style=\"color: #800080; text-decoration-color: #800080\">    1.0040218830108643     </span>â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36m        test-acc         \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[35m \u001b[0m\u001b[35m    0.550000011920929    \u001b[0m\u001b[35m \u001b[0mâ”‚\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36m        test-auc         \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[35m \u001b[0m\u001b[35m   0.43434345722198486   \u001b[0m\u001b[35m \u001b[0mâ”‚\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36m     test-clip_loss      \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[35m \u001b[0m\u001b[35m   2.1738295555114746    \u001b[0m\u001b[35m \u001b[0mâ”‚\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36m      test-cls_loss      \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[35m \u001b[0m\u001b[35m   0.6950899362564087    \u001b[0m\u001b[35m \u001b[0mâ”‚\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36m     test-cls_loss1D     \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[35m \u001b[0m\u001b[35m   0.6951161026954651    \u001b[0m\u001b[35m \u001b[0mâ”‚\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36m     test-cls_loss2D     \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[35m \u001b[0m\u001b[35m   0.6945802569389343    \u001b[0m\u001b[35m \u001b[0mâ”‚\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36m   test-contrast_loss    \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[35m \u001b[0m\u001b[35m   0.3196900486946106    \u001b[0m\u001b[35m \u001b[0mâ”‚\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36m  test-contrast_loss1D   \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[35m \u001b[0m\u001b[35m   0.35999998450279236   \u001b[0m\u001b[35m \u001b[0mâ”‚\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36m  test-contrast_loss2D   \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[35m \u001b[0m\u001b[35m   0.27993908524513245   \u001b[0m\u001b[35m \u001b[0mâ”‚\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36m        test-eer         \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[35m \u001b[0m\u001b[35m   0.6363636255264282    \u001b[0m\u001b[35m \u001b[0mâ”‚\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36m        test-loss        \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[35m \u001b[0m\u001b[35m    4.898554801940918    \u001b[0m\u001b[35m \u001b[0mâ”‚\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36m      test-mse_loss      \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[35m \u001b[0m\u001b[35m   1.0040218830108643    \u001b[0m\u001b[35m \u001b[0mâ”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test-clip_loss': 2.1738295555114746,\n",
       "  'test-mse_loss': 1.0040218830108643,\n",
       "  'test-cls_loss1D': 0.6951161026954651,\n",
       "  'test-cls_loss2D': 0.6945802569389343,\n",
       "  'test-cls_loss': 0.6950899362564087,\n",
       "  'test-contrast_loss': 0.3196900486946106,\n",
       "  'test-contrast_loss1D': 0.35999998450279236,\n",
       "  'test-contrast_loss2D': 0.27993908524513245,\n",
       "  'test-loss': 4.898554801940918,\n",
       "  'test-acc': 0.550000011920929,\n",
       "  'test-auc': 0.43434345722198486,\n",
       "  'test-eer': 0.6363636255264282}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(mvcl_lit, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "Note, train, val, and test process will logging in the same file: `metrics.csv`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "my_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
